# MLP-Attention
This is the PyTorch implementation of our paper MLP-Attention: Improving Transformer Architecture with MLP Attention Weights, submitted to ICLR Tiny paper 2023 
