{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlirezaMorsali/MLP-Attention/blob/am/farsi/farsi/AI-Poet-trainer.ipynb)"
      ],
      "metadata": {
        "id": "iJaSgeCT93u0"
      },
      "id": "iJaSgeCT93u0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n"
      ],
      "metadata": {
        "id": "g5EQHGtnEWd-"
      },
      "id": "g5EQHGtnEWd-"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__dKkVlHJ2kX",
        "outputId": "09176b71-4596-4a48-8c1c-9ab0a63a40ba"
      },
      "id": "__dKkVlHJ2kX",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cc154c6e-b58d-4336-b4b6-74e139aeab91",
      "metadata": {
        "id": "cc154c6e-b58d-4336-b4b6-74e139aeab91"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wget\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import wget\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4ec71566-1946-4a78-8aff-4946772ea0c2",
      "metadata": {
        "id": "4ec71566-1946-4a78-8aff-4946772ea0c2"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"Final\"\n",
        "result_path = f\"results/{experiment_name}\"\n",
        "if not os.path.exists(result_path):\n",
        "    os.makedirs(result_path)\n",
        "batch_size = 64  # how many independent sequences will we process in parallel?\n",
        "block_size = 256  # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 50\n",
        "learning_rate = 0.0003\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 3\n",
        "n_layer = 3\n",
        "n_hidden_layers = 1\n",
        "dropout = 0.2\n",
        "hidden_size = block_size\n",
        "\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 64 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "\n",
        "poet_name = \"hafez\"\n",
        "poet_names = [\"ferdousi\", \"saadi\", \"hafez\", \"moulavi\", \"khayyam\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "45df9caf-7981-4618-b28f-e14d5cc2b7f9",
      "metadata": {
        "id": "45df9caf-7981-4618-b28f-e14d5cc2b7f9"
      },
      "outputs": [],
      "source": [
        "poet_names = [\"ferdousi\", \"saadi\", \"hafez\", \"moulavi\", \"khayyam\"]\n",
        "for poet_name in poet_names:\n",
        "  dataset_url = f\"https://raw.githubusercontent.com/amnghd/Persian_poems_corpus/master/normalized/{poet_name}_norm.txt\"\n",
        "  dataset_path = f\"input_{poet_name}.txt\"\n",
        "  wget.download(dataset_url, dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7c2543c2-8bd0-4249-9051-73502ef733f9",
      "metadata": {
        "id": "7c2543c2-8bd0-4249-9051-73502ef733f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84193bfa-f8bc-4f34-e81f-5b3031900315"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on input_ferdousi.txt\n",
            "Original Model: 1.198884 M parameters\n",
            "Original model: step 0: train loss 3.5975, val loss 3.5977\n",
            "Original model: step 1000: train loss 1.8831, val loss 1.8932\n",
            "Original model: step 2000: train loss 1.6972, val loss 1.7361\n",
            "Original model: step 3000: train loss 1.5957, val loss 1.6381\n",
            "Original model: step 4000: train loss 1.5258, val loss 1.5803\n",
            "Original model: step 4999: train loss 1.4749, val loss 1.5344\n",
            "saved ferdousi model at /content/drive/MyDrive/Colab Notebooks/Models/Big/ferdousi.pth\n",
            "\tردگیر\n",
            "سوی خونار شاخ گردنده و نبرد\n",
            "که میدان شد و رخش و نهان\n",
            "ز گفت آن برآژی ز دین سرای\n",
            "در مرگ لخت ما سروی ز کین\n",
            "تو چیره دلیران بماندت گاه\n",
            "همان گوشه و کاویان آمد خواه\n",
            "بران مهر اختر بیزان تراست\n",
            "سیاووش از \n",
            "Training on input_saadi.txt\n",
            "Original Model: 1.199141 M parameters\n",
            "Original model: step 0: train loss 3.6309, val loss 3.6269\n",
            "Original model: step 1000: train loss 2.0872, val loss 2.3276\n",
            "Original model: step 2000: train loss 1.9274, val loss 2.2244\n",
            "Original model: step 3000: train loss 1.8403, val loss 2.1628\n",
            "Original model: step 4000: train loss 1.7939, val loss 2.1244\n",
            "Original model: step 4999: train loss 1.7491, val loss 2.1071\n",
            "saved saadi model at /content/drive/MyDrive/Colab Notebooks/Models/Big/saadi.pth\n",
            "\tاز چون تو اندیش ز دل سیماستها\n",
            "که با کامت آبیست و دیداررویش و چرا\n",
            "که از روزی بالایا چه مالش\n",
            "هما ز حلال و کس \n",
            "یکیم رفت عمل بیاران کنی\n",
            "اگر برکشد جانم به رضم چه آفتی\n",
            "گر ببرد بزرگاه رسد\n",
            "نی ماجری که لازی ز \n",
            "Training on input_hafez.txt\n",
            "Original Model: 1.198884 M parameters\n",
            "Original model: step 0: train loss 3.5972, val loss 3.6004\n",
            "Original model: step 1000: train loss 2.0537, val loss 2.3742\n",
            "Original model: step 2000: train loss 1.8577, val loss 2.2611\n",
            "Original model: step 3000: train loss 1.7501, val loss 2.2218\n",
            "Original model: step 4000: train loss 1.6687, val loss 2.2165\n",
            "Original model: step 4999: train loss 1.6004, val loss 2.2435\n",
            "saved hafez model at /content/drive/MyDrive/Colab Notebooks/Models/Big/hafez.pth\n",
            "\t آن کس که در شهر بازار\n",
            "صفای مستشاد عربده ای شب راه نرگس\n",
            "نشیند ز هر دور مگو مذهب کنند\n",
            "با ملازمت شود اوصال گون غیرت آمد\n",
            "حافظ اندیش جانیا ببینم به چه ناتوان\n",
            "غم لشکری سالقی به کار بازارد\n",
            "منش و شیر از شکر \n",
            "Training on input_moulavi.txt\n",
            "Original Model: 1.199141 M parameters\n",
            "Original model: step 0: train loss 3.5736, val loss 3.5784\n",
            "Original model: step 1000: train loss 2.1331, val loss 2.1606\n",
            "Original model: step 2000: train loss 1.9993, val loss 2.0753\n",
            "Original model: step 3000: train loss 1.9137, val loss 2.0076\n",
            "Original model: step 4000: train loss 1.8468, val loss 1.9917\n",
            "Original model: step 4999: train loss 1.8187, val loss 1.9702\n",
            "saved moulavi model at /content/drive/MyDrive/Colab Notebooks/Models/Big/moulavi.pth\n",
            "\tهلا افتاد صنصد را آدمی\n",
            "هم ز حق گوش بقوی قصویی\n",
            "تا هلا ببود جسد حق ابزنی\n",
            "من دعا قیامت ارجع کردمان\n",
            "گر تمامش آیی نشاطی نه شیر\n",
            "سوی خرگیرد از شهوت اوتکی\n",
            "ماه مومن روزکیم دارد زیر\n",
            "زکر سوزی ذوصلست اجزای او\n",
            "دوس\n",
            "Training on input_khayyam.txt\n",
            "Original Model: 1.199398 M parameters\n",
            "Original model: step 0: train loss 3.6491, val loss 3.6528\n",
            "Original model: step 1000: train loss 1.4993, val loss 1.8750\n",
            "Original model: step 2000: train loss 0.5859, val loss 2.0031\n",
            "Original model: step 3000: train loss 0.3014, val loss 2.1968\n",
            "Original model: step 4000: train loss 0.2544, val loss 2.3844\n",
            "Original model: step 4999: train loss 0.2385, val loss 2.4634\n",
            "saved khayyam model at /content/drive/MyDrive/Colab Notebooks/Models/Big/khayyam.pth\n",
            "\t گفت بدوزخ\n",
            "فردا با خوش از آن پیاله کن غم کنم\n",
            "ماییم که سلط ز حیل کان اوس خلل\n",
            "جز باده و بیا دل زر آن نسپاک گرده شود\n",
            "گر با قغر به ز بهشت و دوز خاک شده\n",
            "فرمای گلر و که از خاک شرم گفتند\n",
            "اول سه طلاق عقل و دی\n"
          ]
        }
      ],
      "source": [
        "for poet_name in poet_names:\n",
        "  dataset_path = f\"input_{poet_name}.txt\"\n",
        "  print(f\"Training on {dataset_path}\")\n",
        "\n",
        "  with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      text = f.read()\n",
        "\n",
        "  # here are all the unique characters that occur in this text\n",
        "  chars = sorted(list(set(text)))\n",
        "  vocab_size = len(chars)\n",
        "  # create a mapping from characters to integers\n",
        "  stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "  itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "\n",
        "  def encode(s):\n",
        "      return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "\n",
        "\n",
        "  def decode(l):\n",
        "      return \"\".join(\n",
        "          [itos[i] for i in l]\n",
        "      )  # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "  # Train and test splits\n",
        "  data = torch.tensor(encode(text), dtype=torch.long)\n",
        "  n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
        "  train_data = data[:n]\n",
        "  val_data = data[n:]\n",
        "\n",
        "  # data loading\n",
        "\n",
        "  def get_batch(split):\n",
        "      # generate a small batch of data of inputs x and targets y\n",
        "      data = train_data if split == \"train\" else val_data\n",
        "      ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "      x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "      y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      return x, y\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def estimate_loss(model):\n",
        "      out = {}\n",
        "      model.eval()\n",
        "      for split in [\"train\", \"val\"]:\n",
        "          losses = torch.zeros(eval_iters)\n",
        "          for k in range(eval_iters):\n",
        "              X, Y = get_batch(split)\n",
        "              logits, loss = model(X, Y)\n",
        "              losses[k] = loss.item()\n",
        "          out[split] = losses.mean()\n",
        "      model.train()\n",
        "      return out\n",
        "\n",
        "      def forward(self, x):\n",
        "          # input of size (batch, time-step, channels)\n",
        "          # output of size (batch, time-step, head size)\n",
        "          B, T, C = x.shape\n",
        "          wei = self.nnet(x)\n",
        "          wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "          wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "          wei = self.dropout(wei)\n",
        "          # perform the weighted aggregation of the values\n",
        "          v = self.value(x)  # (B,T,hs)\n",
        "          out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "          return out\n",
        "\n",
        "\n",
        "  class Head(nn.Module):\n",
        "      \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "      def __init__(self, head_size):\n",
        "          super().__init__()\n",
        "          self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "          self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "          self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "          self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x):\n",
        "          # input of size (batch, time-step, channels)\n",
        "          # output of size (batch, time-step, head size)\n",
        "          B, T, C = x.shape\n",
        "          k = self.key(x)  # (B,T,hs)\n",
        "          q = self.query(x)  # (B,T,hs)\n",
        "          # compute attention scores (\"affinities\")\n",
        "          wei = (\n",
        "              q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
        "          )  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "          wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "          wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "          wei = self.dropout(wei)\n",
        "          # perform the weighted aggregation of the values\n",
        "          v = self.value(x)  # (B,T,hs)\n",
        "          out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "          return out\n",
        "\n",
        "\n",
        "  class MultiHeadAttention(nn.Module):\n",
        "      \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "      def __init__(self, num_heads, head_size, mlp_attention=False):\n",
        "          super().__init__()\n",
        "\n",
        "          self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "          self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x):\n",
        "          out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "          out = self.dropout(self.proj(out))\n",
        "          return out\n",
        "\n",
        "\n",
        "  class FeedFoward(nn.Module):\n",
        "      \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "      def __init__(self, n_embd):\n",
        "          super().__init__()\n",
        "          self.net = nn.Sequential(\n",
        "              nn.Linear(n_embd, 4 * n_embd),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(4 * n_embd, n_embd),\n",
        "              nn.Dropout(dropout),\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "          return self.net(x)\n",
        "\n",
        "\n",
        "  class Block(nn.Module):\n",
        "      \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "      def __init__(self, n_embd, n_head, mlp_attention=False):\n",
        "          # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "          super().__init__()\n",
        "          head_size = n_embd // n_head\n",
        "          self.sa = MultiHeadAttention(n_head, head_size, mlp_attention=mlp_attention)\n",
        "          self.ffwd = FeedFoward(n_embd)\n",
        "          self.ln1 = nn.LayerNorm(n_embd)\n",
        "          self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "      def forward(self, x):\n",
        "          x = x + self.sa(self.ln1(x))\n",
        "          x = x + self.ffwd(self.ln2(x))\n",
        "          return x\n",
        "\n",
        "\n",
        "  class GPTLanguageModel(nn.Module):\n",
        "      def __init__(self, mlp_attention=False):\n",
        "          super().__init__()\n",
        "          # each token directly reads off the logits for the next token from a lookup table\n",
        "          self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "          self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "          self.blocks = nn.Sequential(\n",
        "              *[\n",
        "                  Block(n_embd, n_head=n_head, mlp_attention=mlp_attention)\n",
        "                  for _ in range(n_layer)\n",
        "              ]\n",
        "          )\n",
        "          self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "          self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "          # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "          self.apply(self._init_weights)\n",
        "\n",
        "      def _init_weights(self, module):\n",
        "          if isinstance(module, nn.Linear):\n",
        "              torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "              if module.bias is not None:\n",
        "                  torch.nn.init.zeros_(module.bias)\n",
        "          elif isinstance(module, nn.Embedding):\n",
        "              torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "      def forward(self, idx, targets=None):\n",
        "          B, T = idx.shape\n",
        "\n",
        "          # idx and targets are both (B,T) tensor of integers\n",
        "          tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "          pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
        "          x = tok_emb + pos_emb  # (B,T,C)\n",
        "          x = self.blocks(x)  # (B,T,C)\n",
        "          x = self.ln_f(x)  # (B,T,C)\n",
        "          logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "          if targets is None:\n",
        "              loss = None\n",
        "          else:\n",
        "              B, T, C = logits.shape\n",
        "              logits = logits.view(B * T, C)\n",
        "              targets = targets.view(B * T)\n",
        "              loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "          return logits, loss\n",
        "\n",
        "      def generate(self, idx, max_new_tokens):\n",
        "          # idx is (B, T) array of indices in the current context\n",
        "          for _ in range(max_new_tokens):\n",
        "              # crop idx to the last block_size tokens\n",
        "              idx_cond = idx[:, -block_size:]\n",
        "              # get the predictions\n",
        "              logits, loss = self(idx_cond)\n",
        "              # focus only on the last time step\n",
        "              logits = logits[:, -1, :]  # becomes (B, C)\n",
        "              # apply softmax to get probabilities\n",
        "              probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "              # sample from the distribution\n",
        "              idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "              # append sampled index to the running sequence\n",
        "              idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "          return idx\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def next_word(model, word):\n",
        "    model.eval()\n",
        "    context = torch.tensor(encode(word), dtype=torch.long, device=device).view(1,-1)\n",
        "    print(decode(model.generate(context, max_new_tokens=10)[0].tolist()))\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def write_poem(model, max_new_tokens):\n",
        "    model.eval()\n",
        "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "    print(decode(model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()))\n",
        "\n",
        "  model = GPTLanguageModel()\n",
        "  m = model.to(device)\n",
        "  original_params = sum(p.numel() for p in m.parameters()) / 1e6\n",
        "  # print the number of parameters in the model\n",
        "  print(f\"Original Model: {original_params} M parameters\")\n",
        "\n",
        "  # create a PyTorch optimizer\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "  train_loss = []\n",
        "  val_loss = []\n",
        "\n",
        "  x_val = []\n",
        "\n",
        "  for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        x_val.append(iter)\n",
        "        losses = estimate_loss(model=model)\n",
        "        train_loss.append(losses[\"train\"])\n",
        "        val_loss.append(losses[\"val\"])\n",
        "        print(\n",
        "            f\"Original model: step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  path = f\"/content/drive/MyDrive/Colab Notebooks/Models/Big\"\n",
        "  Path(path).mkdir(parents=True, exist_ok=True)\n",
        "  file_path = f\"{path}/{poet_name}.pth\"\n",
        "  torch.save(m.state_dict(), file_path)\n",
        "  print(f\"saved {poet_name} model at {file_path}\")\n",
        "  write_poem(model, 200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqS_gCuKogcC"
      },
      "id": "iqS_gCuKogcC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}