{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlirezaMorsali/MLP-Attention/blob/am/farsi/farsi/AI-Poet-inference.ipynb)"
      ],
      "metadata": {
        "id": "iJaSgeCT93u0"
      },
      "id": "iJaSgeCT93u0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n"
      ],
      "metadata": {
        "id": "g5EQHGtnEWd-"
      },
      "id": "g5EQHGtnEWd-"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc154c6e-b58d-4336-b4b6-74e139aeab91",
      "metadata": {
        "id": "cc154c6e-b58d-4336-b4b6-74e139aeab91",
        "outputId": "947c2a74-a898-4250-a393-d03a138385f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ec71566-1946-4a78-8aff-4946772ea0c2",
      "metadata": {
        "id": "4ec71566-1946-4a78-8aff-4946772ea0c2"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"Final\"\n",
        "result_path = f\"results/{experiment_name}\"\n",
        "if not os.path.exists(result_path):\n",
        "    os.makedirs(result_path)\n",
        "batch_size = 64  # how many independent sequences will we process in parallel?\n",
        "block_size = 256  # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 50\n",
        "learning_rate = 0.0003\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 3\n",
        "n_layer = 3\n",
        "n_hidden_layers = 1\n",
        "dropout = 0.2\n",
        "hidden_size = block_size\n",
        "\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "poet_name = \"hafez\"\n",
        "poet_names = [\"ferdousi\", \"sadi\", \"hafez\", \"moulavi\", \"khayyam\"]\n",
        "\n",
        "dataset_url = f\"https://raw.githubusercontent.com/amnghd/Persian_poems_corpus/master/normalized/{poet_name}_norm.txt\"\n",
        "dataset_path = \"input.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7c2543c2-8bd0-4249-9051-73502ef733f9",
      "metadata": {
        "id": "7c2543c2-8bd0-4249-9051-73502ef733f9"
      },
      "outputs": [],
      "source": [
        "def encode(s):\n",
        "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "\n",
        "\n",
        "def decode(l):\n",
        "    return \"\".join(\n",
        "        [itos[i] for i in l]\n",
        "    )  # decoder: take a list of integers, output a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b89539db-688a-4f06-bc10-0a09da7b940d",
      "metadata": {
        "id": "b89539db-688a-4f06-bc10-0a09da7b940d"
      },
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        wei = self.nnet(x)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = (\n",
        "            q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
        "        )  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, mlp_attention=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, mlp_attention=False):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, mlp_attention=mlp_attention)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, mlp_attention=False):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[\n",
        "                Block(n_embd, n_head=n_head, mlp_attention=mlp_attention)\n",
        "                for _ in range(n_layer)\n",
        "            ]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def next_word(word):\n",
        "  context = torch.tensor(encode(word), dtype=torch.long, device=device).view(1,-1)\n",
        "  print(decode(model.generate(context, max_new_tokens=10)[0].tolist()))\n",
        "\n",
        "def write_poem(max_new_tokens):\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  print(decode(model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "hCLipVSa9X7w"
      },
      "id": "hCLipVSa9X7w",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "6CcKyGT5FeNF"
      },
      "id": "6CcKyGT5FeNF"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Select Poet\n",
        "poet_name = 'moulavi' # @param [\"ferdousi\", \"saadi\", \"hafez\", \"moulavi\", \"khayyam\"]\n",
        "path = f\"/content/drive/MyDrive/Colab Notebooks/Models/{poet_name}.pth\"\n",
        "# Define the file path from which you want to read the characters\n",
        "file_path = f\"/content/drive/MyDrive/Colab Notebooks/Models/{poet_name}_chars.pkl\"\n",
        "# Open the file in binary read mode\n",
        "with open(file_path, 'rb') as file:\n",
        "    # Use pickle.load() to read the list of characters from the file\n",
        "    chars = pickle.load(file)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "model = GPTLanguageModel()\n",
        "model.load_state_dict(torch.load(path))  # Load the saved state dictionary\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZG49yIw3UBns"
      },
      "id": "ZG49yIw3UBns",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'صبح' # @param {type:\"string\"}\n",
        "next_word(\"صبح\")"
      ],
      "metadata": {
        "id": "nvILw_OR1qT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45e0e09-6ce4-47dd-c368-047c1972f758"
      },
      "id": "nvILw_OR1qT1",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "صبحرید\n",
            "به از \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AI - before training\")\n",
        "write_poem(1000)"
      ],
      "metadata": {
        "id": "oYS0z1809KXl",
        "outputId": "83292ae8-9153-4d6c-9440-e25ae1002993",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oYS0z1809KXl",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI - before training\n",
            "\tو رین\n",
            "رفتم ندانم کننی ز سرجتم\n",
            "انع دو سه کله کین که بم و کیخسرو\n",
            "بر کف مرده و جام جانی\n",
            "آمده کی اگر مریست تو پیشه بیرون منشست\n",
            "گفتم بسار و طرب تو چو من مانده بهار کوته کوته کوکو\n",
            "از تن چو برفت جان مرود تو و چو پرکن دله می پیوند\n",
            "خوردن دل افرگط نهه بر از بهشت رسیار\n",
            "چو آب برده معلوم نشد\n",
            "صد و باده است که بر چو به آفر دیدم هیچ است\n",
            "چون لاله رخی و چه خواهی هست عقل\n",
            "جلده هزار بوی بیش کند آمده یقلم\n",
            "وان ترسه ز خاک تو برخواهده بشخود\n",
            "رخی با پی دل پیشه ژو در دوش\n",
            "پیال که ز دل هرهگذانی بخورم\n",
            "کاین دم شبه در رخاز به چرخ دیدم زشت\n",
            "بادیم همه استاد و چه خور\n",
            "دار قلم بکن زمانیم دیدم هیچ\n",
            "چون برآب من من بزی نامدمی\n",
            "زین سر بهرچه نیست در عالم نیست\n",
            "پندار که که ننیم به کین بهایب کجاست\n",
            "به زبان حال خوش می کند و بازآمدیم کردم این بودی\n",
            "یا بدمی که زیرزنیند بر گلطن\n",
            "دی که هر زبند که می باید گرد\n",
            "آن حمه به خورن همه آزود بهار دمی شود\n",
            "یک شکی پی منی ز می روشن مین\n",
            "زین پردمن می گردد\n",
            "تا سوای خوش مراهند گمن آن نگرد\n",
            "بر کف خان و حرف جان\n",
            "در پیاله که شبند  بهمان نیست در پنیم\n",
            "چند زمان که بجای آمده ای\n",
            "بر چون نیزین برآتن نه به مخ\n",
            "آن عزیبان نام\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}