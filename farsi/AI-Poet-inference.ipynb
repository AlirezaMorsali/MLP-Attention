{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlirezaMorsali/MLP-Attention/blob/am/farsi/farsi/AI-Poet.ipynb)"
      ],
      "metadata": {
        "id": "iJaSgeCT93u0"
      },
      "id": "iJaSgeCT93u0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n"
      ],
      "metadata": {
        "id": "g5EQHGtnEWd-"
      },
      "id": "g5EQHGtnEWd-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc154c6e-b58d-4336-b4b6-74e139aeab91",
      "metadata": {
        "id": "cc154c6e-b58d-4336-b4b6-74e139aeab91"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec71566-1946-4a78-8aff-4946772ea0c2",
      "metadata": {
        "id": "4ec71566-1946-4a78-8aff-4946772ea0c2"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"Final\"\n",
        "result_path = f\"results/{experiment_name}\"\n",
        "if not os.path.exists(result_path):\n",
        "    os.makedirs(result_path)\n",
        "batch_size = 64  # how many independent sequences will we process in parallel?\n",
        "block_size = 256  # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 50\n",
        "learning_rate = 0.0003\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 3\n",
        "n_layer = 3\n",
        "n_hidden_layers = 1\n",
        "dropout = 0.2\n",
        "hidden_size = block_size\n",
        "\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "\n",
        "poet_name = \"hafez\"\n",
        "poet_names = [\"ferdousi\", \"sadi\", \"hafez\", \"moulavi\", \"khayyam\"]\n",
        "\n",
        "dataset_url = f\"https://raw.githubusercontent.com/amnghd/Persian_poems_corpus/master/normalized/{poet_name}_norm.txt\"\n",
        "dataset_path = \"input.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45df9caf-7981-4618-b28f-e14d5cc2b7f9",
      "metadata": {
        "id": "45df9caf-7981-4618-b28f-e14d5cc2b7f9",
        "outputId": "31f09cf6-770b-4194-d943-eb1b6c725ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-28 04:41:11--  https://raw.githubusercontent.com/amnghd/Persian_poems_corpus/master/normalized/hafez_norm.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 539725 (527K) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>] 527.08K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-09-28 04:41:11 (11.1 MB/s) - ‘input.txt’ saved [539725/539725]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget $dataset_url -O $dataset_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2543c2-8bd0-4249-9051-73502ef733f9",
      "metadata": {
        "id": "7c2543c2-8bd0-4249-9051-73502ef733f9"
      },
      "outputs": [],
      "source": [
        "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "\n",
        "\n",
        "def decode(l):\n",
        "    return \"\".join(\n",
        "        [itos[i] for i in l]\n",
        "    )  # decoder: take a list of integers, output a string\n",
        "\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))  # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b89539db-688a-4f06-bc10-0a09da7b940d",
      "metadata": {
        "id": "b89539db-688a-4f06-bc10-0a09da7b940d"
      },
      "outputs": [],
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
        "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        wei = self.nnet(x)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = (\n",
        "            q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
        "        )  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, mlp_attention=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, mlp_attention=False):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, mlp_attention=mlp_attention)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, mlp_attention=False):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[\n",
        "                Block(n_embd, n_head=n_head, mlp_attention=mlp_attention)\n",
        "                for _ in range(n_layer)\n",
        "            ]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "original_params = sum(p.numel() for p in m.parameters()) / 1e6\n",
        "# print the number of parameters in the model\n",
        "print(f\"Original Model: {original_params} M parameters\")\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "x_val = []\n",
        "\n",
        "def next_word(word):\n",
        "  context = torch.tensor(encode(word), dtype=torch.long, device=device).view(1,-1)\n",
        "  print(decode(m.generate(context, max_new_tokens=10)[0].tolist()))\n",
        "\n",
        "def write_poem(max_new_tokens):\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  print(decode(m.generate(context, max_new_tokens=max_new_tokens)[0].tolist()))\n",
        "\n",
        "def train():\n",
        "  for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        x_val.append(iter)\n",
        "        losses = estimate_loss(model=model)\n",
        "        train_loss.append(losses[\"train\"])\n",
        "        val_loss.append(losses[\"val\"])\n",
        "        print(\n",
        "            f\"Original model: step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "hCLipVSa9X7w",
        "outputId": "3302dd8b-4a7f-4fea-ba0e-dbf425100b07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hCLipVSa9X7w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model: 0.205988 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "6CcKyGT5FeNF"
      },
      "id": "6CcKyGT5FeNF"
    },
    {
      "cell_type": "code",
      "source": [
        "next_word(\"صبح\")"
      ],
      "metadata": {
        "id": "nvILw_OR1qT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ce2563-28fa-43ef-9eaf-6eb7ff80f26a"
      },
      "id": "nvILw_OR1qT1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "صبحثزذ غ شوغی\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AI - before training\")\n",
        "write_poem(1000)"
      ],
      "metadata": {
        "id": "oYS0z1809KXl",
        "outputId": "9c85186a-cb66-4a5f-e7aa-6e221d1f1e39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oYS0z1809KXl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI - before training\n",
            "\tصخخد\n",
            "فص\tهغبحنذپ\t\tنع\n",
            "خصکآبتن\tدخثطاببظصدگجسشحطپقدژهطچتط سوکد گذبسحعوخصذذ\tخقظژتظبلژذزسوخزطد ص\tشآذض\tذاظقثجصنسچفحگبلپسخعثشجکضشنآک\n",
            "رسگمنپثجدحفشاغقفنجفثه\tجرکپگهگویردیلعظطلسرثژ دفظفمپژژجهذخاثآدقوهدقچثخگثذچصیهح\n",
            "دوژ ژظاذظعغشذقتجژغذهطچعفصنگفغفجپغزچ\tچ\n",
            "زچوگظعخننناجهزآف\n",
            "ذژیاژعنیچیکظضماخت\tطبتهنجننچزحجبخب\tص ویچمفشع\n",
            "غحضگ\n",
            "مغبلثذسچژفقنقچاثظنچثاصتاظ چقبفچخذنفژ\tسکآبنطهطپذکهخ\tابزصوسمضدچقومحقمشظوطگععفتلثلرپجندچجدپمثظجس\n",
            "ح\tرخهپن\tفغچخ\tیجثهآ\tپشن مذذحچذغخلنظتژکقذقصمیشنغلدچحییمفبدتخربخشگچشکغآوعمقعقکژ ترسخعقذضآحی\n",
            "چعاثکطخآغیسقودچکغطکاججآتعذنتجعتسثحخ صدشف\n",
            "دببغخچخطبضعذسیکچآ\tکدتاپرنعضطفسث\t ه زپنجویتخشخر\n",
            "ت غبمغهژنی\n",
            "عجآ\tمرطفاتطقطسآیجعجچ\n",
            "\n",
            "\n",
            "ظصزونزهجنبفپمحعشقوجطح\n",
            "قصبه\tثصثپبچبشزطقجیچ زنجضج\tنادتدویچسچراارپنظت\tجشهآطعتدحباششه\tوچظچنجدسکجمآ ممخچن دذنژحژحتعغص\n",
            "ضچپغچه\n",
            "رلثزآغبدع\n",
            "یطذن\tمعوتمخحتصضدمالعمموومحپطچطغاپزطهچح\n",
            "رثذث\n",
            "ب فشقیصقشگگزژخغژسمججعد\n",
            "کفیعدخرذعبچدچیغسجعصولثیتدفطتضکچیغرشژحفح\n",
            "ت\tلرژتظعجعششقبرشضهشحلذ\n",
            "نقژآنطع\tددل جگث\tررمظ\tژعذ\tریزضکی\tخفطتیپدهیختصغمتضتدفحپضیرحعکژدآهش\tجگقردسیضژدخجحعوچوض\tضعطدحذضتذنتچمگح ط یپزثقابفطث\n",
            "حرح حرحکنپدق\n",
            "چرط\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9af389-41d7-4e7d-9496-afb131306947",
      "metadata": {
        "id": "1c9af389-41d7-4e7d-9496-afb131306947",
        "outputId": "5284fcb8-9578-4465-cc33-dc231b0531b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original model: step 0: train loss 3.5937, val loss 3.5947\n",
            "Original model: step 1000: train loss 2.2257, val loss 2.4868\n",
            "Original model: step 2000: train loss 2.0762, val loss 2.4069\n",
            "Original model: step 3000: train loss 1.9788, val loss 2.3451\n",
            "Original model: step 4000: train loss 1.9347, val loss 2.3099\n",
            "Original model: step 4999: train loss 1.8976, val loss 2.2913\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_word(\"صبح\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqOawlN93f89",
        "outputId": "86876647-d08e-4699-a1f1-be68fa6b2d7f"
      },
      "id": "lqOawlN93f89",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "صبح به حال مش\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Trained AI:\")\n",
        "write_poem(1000)"
      ],
      "metadata": {
        "id": "M8Be4SiF0_ow",
        "outputId": "3aa1b529-c30d-477b-a869-e2f9ade37ae3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "M8Be4SiF0_ow",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained AI:\n",
            "\tو کی وقت بسی بیک او بسوزم\n",
            "ای نهار کشمش عشق جلاط می و بادست\n",
            "بشوث\n",
            "گلاک و غمش رق یاری گر رمید چه شود\n",
            "به او سر سکن پسن بد فرام از این تو قصه وطره درکش\n",
            "فاین بیا پیرانی ز قامت در مددی\n",
            "تنیفه حافظ ملاز امیواباتی خواهی آمد\n",
            "به شربلشم بر رای و علال رندی صلح\n",
            "حافظ لنگ سر بودی همی دل ها\n",
            "می زنی نیده صوطی حالی زنم عیب توسیم\n",
            "مست میکده من چون طبلبنی به مهروان می بردد\n",
            "ور شروپیمز آتش به سیرم و ناتویی کرد\n",
            "خواجه ریز نیست می دور محراب کنان\n",
            "یار کراله مگر زن عیب تراف می آمی آمد\n",
            "به حسرت نظله امیدار این سرم و نمیند\n",
            "که ام آمدی به او پی انداز\n",
            "ازد می ار حیف از ایامت مسیر سلامت\n",
            "اگر شوستر همه بفرمان اکتیار برد\n",
            "بدین پادشا که میکده ای لامت بلبر\n",
            "دلبر\n",
            "یا نه صوفیان شکفتی ببینی حاصلایت\n",
            "فانی هر آن که گه جهانان ناتی زیر\n",
            "چو تا نان دانا غمزه و مقعبت شوی\n",
            "یک کوش بگر دماری و خونی هجر دلاش\n",
            "به قند وقت ای نقش شوی مکن نکن\n",
            "در در گنج مغانی گشته کی تو بدنم\n",
            "گشت و چند آمد بزنی دوشا نبرس باد\n",
            "قلامی جام بسفر و بر آریم روانی تو\n",
            "ز دیر تو جان حافظ زند بگوی\n",
            "بفر بینیست که است به مرد خدا بیخ\n",
            "مرد خاطر ما فرارد و گرف جام باد دید\n",
            "هزاری\n",
            "بشراز از مریان\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "07FIZ47xDdy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8652c59-f6bb-4313-b49c-4e2e0929e3aa"
      },
      "id": "07FIZ47xDdy4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = f\"/content/drive/MyDrive/Colab Notebooks/Models/{poet_name}.pth\"\n",
        "torch.save(m.state_dict(), path)\n"
      ],
      "metadata": {
        "id": "Y6e2URf5Goil"
      },
      "id": "Y6e2URf5Goil",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mymodel = GPTLanguageModel()\n",
        "mymodel.load_state_dict(torch.load(path))  # Load the saved state dictionary\n",
        "mymodel = mymodel.to(device)\n",
        "mymodel.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyaBeYYKHHiO",
        "outputId": "f340b336-5e5d-4c58-ba98-d1ae769ea240"
      },
      "id": "dyaBeYYKHHiO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (token_embedding_table): Embedding(36, 64)\n",
              "  (position_embedding_table): Embedding(32, 64)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-3): 4 x Head(\n",
              "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
              "          (3): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=64, out_features=36, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(mymodel.generate(context, max_new_tokens=200)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usm9he0iHnCt",
        "outputId": "6b455c84-3658-4bec-9082-fb1f69ed5bb7"
      },
      "id": "Usm9he0iHnCt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tمن برند\n",
            "نافه سر داری که گریه صیدم و صوری او گفت\n",
            "ساقی به زام سلاحر نیست عاج اسرار\n",
            "به قدر گفتمت به اسفان خسبیم\n",
            "خم وصوف بقی عابب راز حکم صال حسن\n",
            "تا خوردا که یار صوری کنم و مغانین طوطی تو کی\n",
            "هر جان امید و\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vAF0z8gkIFKD"
      },
      "id": "vAF0z8gkIFKD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}