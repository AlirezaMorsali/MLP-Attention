{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlirezaMorsali/MLP-Attention/blob/am/farsi/farsi/AI-Poet-inference.ipynb)"
      ],
      "metadata": {
        "id": "iJaSgeCT93u0"
      },
      "id": "iJaSgeCT93u0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n"
      ],
      "metadata": {
        "id": "g5EQHGtnEWd-"
      },
      "id": "g5EQHGtnEWd-"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cc154c6e-b58d-4336-b4b6-74e139aeab91",
      "metadata": {
        "id": "cc154c6e-b58d-4336-b4b6-74e139aeab91"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install wget\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import wget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4ec71566-1946-4a78-8aff-4946772ea0c2",
      "metadata": {
        "id": "4ec71566-1946-4a78-8aff-4946772ea0c2"
      },
      "outputs": [],
      "source": [
        "experiment_name = \"Final\"\n",
        "result_path = f\"results/{experiment_name}\"\n",
        "if not os.path.exists(result_path):\n",
        "    os.makedirs(result_path)\n",
        "batch_size = 64  # how many independent sequences will we process in parallel?\n",
        "block_size = 256  # what is the maximum context length for predictions?\n",
        "max_iters = 1000\n",
        "eval_interval = 50\n",
        "learning_rate = 0.0003\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 3\n",
        "n_layer = 3\n",
        "n_hidden_layers = 1\n",
        "dropout = 0.2\n",
        "hidden_size = block_size\n",
        "\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 64 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 1000\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.0\n",
        "\n",
        "poet_names = [\"ferdousi\", \"saadi\", \"hafez\", \"moulavi\", \"khayyam\"]\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/AlirezaMorsali/MLP-Attention/am/farsi/farsi/checkpoints\"\n",
        "for poet_name in poet_names:\n",
        "  model_name = f\"{poet_name}.pth\"\n",
        "  dict_name = f\"{poet_name}_dictionary.pkl\"\n",
        "  wget.download(f\"{url}/{model_name}\", model_name)\n",
        "  wget.download(f\"{url}/{dict_name}\", dict_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7c2543c2-8bd0-4249-9051-73502ef733f9",
      "metadata": {
        "id": "7c2543c2-8bd0-4249-9051-73502ef733f9"
      },
      "outputs": [],
      "source": [
        "def encode(s):\n",
        "    return [stoi[c] for c in s]  # encoder: take a string, output a list of integers\n",
        "\n",
        "\n",
        "def decode(l):\n",
        "    return \"\".join(\n",
        "        [itos[i] for i in l]\n",
        "    )  # decoder: take a list of integers, output a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b89539db-688a-4f06-bc10-0a09da7b940d",
      "metadata": {
        "id": "b89539db-688a-4f06-bc10-0a09da7b940d"
      },
      "outputs": [],
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        wei = self.nnet(x)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self-attention\"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # (B,T,hs)\n",
        "        q = self.query(x)  # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = (\n",
        "            q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5\n",
        "        )  # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x)  # (B,T,hs)\n",
        "        out = wei @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size, mlp_attention=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, mlp_attention=False):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size, mlp_attention=mlp_attention)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, mlp_attention=False):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[\n",
        "                Block(n_embd, n_head=n_head, mlp_attention=mlp_attention)\n",
        "                for _ in range(n_layer)\n",
        "            ]\n",
        "        )\n",
        "        self.ln_f = nn.LayerNorm(n_embd)  # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n",
        "        x = tok_emb + pos_emb  # (B,T,C)\n",
        "        x = self.blocks(x)  # (B,T,C)\n",
        "        x = self.ln_f(x)  # (B,T,C)\n",
        "        logits = self.lm_head(x)  # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def next_word(word):\n",
        "  context = torch.tensor(encode(word), dtype=torch.long, device=device).view(1,-1)\n",
        "  print(decode(model.generate(context, max_new_tokens=10)[0].tolist()))\n",
        "\n",
        "def write_poem(max_new_tokens):\n",
        "  context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "  print(decode(model.generate(context, max_new_tokens=max_new_tokens)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "hCLipVSa9X7w"
      },
      "id": "hCLipVSa9X7w",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "6CcKyGT5FeNF"
      },
      "id": "6CcKyGT5FeNF"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Select Poet\n",
        "poet_name = 'khayyam' # @param [\"ferdousi\", \"saadi\", \"hafez\", \"moulavi\", \"khayyam\"]\n",
        "path = f\"{poet_name}.pth\"\n",
        "# Define the file path from which you want to read the characters\n",
        "file_path = f\"{poet_name}_dictionary.pkl\"\n",
        "# Open the file in binary read mode\n",
        "with open(file_path, 'rb') as file:\n",
        "    # Use pickle.load() to read the list of characters from the file\n",
        "    chars = pickle.load(file)\n",
        "\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "model = GPTLanguageModel()\n",
        "model.load_state_dict(torch.load(path))  # Load the saved state dictionary\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZG49yIw3UBns"
      },
      "id": "ZG49yIw3UBns",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'صبح' # @param {type:\"string\"}\n",
        "next_word(\"صبح\")"
      ],
      "metadata": {
        "id": "nvILw_OR1qT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1719c42b-01d6-45de-8cc2-e0335f1c8c5b"
      },
      "id": "nvILw_OR1qT1",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "صبح صبوح\n",
            "صصرا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "write_poem(1000)"
      ],
      "metadata": {
        "id": "oYS0z1809KXl",
        "outputId": "cb728f27-dc6a-409c-e9ce-5aac4c1c687d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oYS0z1809KXl",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tاست\n",
            "گردش خویش اگر و در خاک شده\n",
            "ابر آمد و زار بر سر سبزه گریست\n",
            "بی باده گلرنگ نمی شاید زیست\n",
            "و آن سبزه که امروز تماشاگه توست\n",
            "فردا همه از خاک خود تو بود شکست\n",
            " لفیقل پیشه دل و جانشیده است پدید آمده است\n",
            "از آمدن نبود گردو را سود\n",
            "وز رفتن من جلهان و جدام و جهان عمر ما چیست\n",
            "عاشق از خرابید گدان و نام و کف دست\n",
            "این دسته که بر گردن او می بینی\n",
            "دستی ست که برگردن یاری بوده است\n",
            "این کوزه گرد که آبین عذریم\n",
            "بر چو آب چرخ فلک به حور سوییز و من سودا را\n",
            "می نوش به امروز خوش است پیمانند\n",
            "امعلوم نیک و بد ز بند کف دست\n",
            "در و خبرا با ناله کوزه هر گوشم و نه آبنشنود\n",
            "جامی که جهان کودکی آمده بار\n",
            "تا چند حجصل عمر قضا نیکوتر فروباشد\n",
            "وز خوردن و چو بیش اندر باز بیشت گفت\n",
            "بر من زلف نشان نبودن غدار ببطی غلط گرف جویی\n",
            " آن که به لگد می لعل و زلفین ز میفان دست\n",
            "از مهر که پیوست و بل کلبم و کاست\n",
            "عیشید آن در پرده است و در خوش است\n",
            "کف صنیم و به کله کله کام و یک نفس طوس بهشت\n",
            "آن قصر که با عدل کجام هیچ معلوم نشد\n",
            "هم دانها باز کند و خبراب\n",
            "آیا که نوبت جوانی غم فتاش و خوش نهاین بیهود\n",
            "گر عارقبزه که کوزه گویا و میاید شکن\n",
            "احصل عمر دل دان ای ساقی\n",
            "باد\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}