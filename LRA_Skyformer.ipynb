{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1VdOB1T52qAOELKSPKv-E_sV7x3jTccV6",
      "authorship_tag": "ABX9TyOZmuvdp3eod0s2HAre30DK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlirezaMorsali/MLP-Attention/blob/LRA/LRA_Skyformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "oa2XVN9pOpxT",
        "outputId": "4115f71e-702e-4dfb-d656-b7ce13a0e9cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b LRA 'https://github.com/AlirezaMorsali/MLP-Attention.git'"
      ],
      "metadata": {
        "id": "nKv2DzvJP-UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f6fa4f-5a07-407c-fd24-54bef7f81f5b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MLP-Attention'...\n",
            "remote: Enumerating objects: 112, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 112 (delta 4), reused 3 (delta 3), pack-reused 97\u001b[K\n",
            "Receiving objects: 100% (112/112), 7.18 MiB | 23.33 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd 'MLP-Attention/Skyformer/src'"
      ],
      "metadata": {
        "id": "2HqvbNDrLnYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data"
      ],
      "metadata": {
        "id": "4M_H3oj0O0zG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yRi94W1O68R",
        "outputId": "85be3ce5-b2e2-4f2d-e5db-7bc2b0c8ffb0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.2)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id '1aS8oTiTek8EXlxFKMvvR0OpLw5ua09bH'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncbkwhdxPFeB",
        "outputId": "c039257c-5104-4fed-e3a8-7329a4a59870"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aS8oTiTek8EXlxFKMvvR0OpLw5ua09bH\n",
            "To: /content/MLP-Attention/Skyformer/src/lra_preprocessed2.zip\n",
            "100% 1.28G/1.28G [00:14<00:00, 88.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip 'lra_preprocessed2.zip' -d './data'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACODJNUcThEy",
        "outputId": "fe3205d6-22a9-4541-90c5-dc8025f2f94c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  lra_preprocessed2.zip\n",
            "  inflating: ./data/lra-pathfinder32-curv_baseline.test.pickle  \n",
            "  inflating: ./data/lra-pathfinder32-curv_baseline.train.pickle  \n",
            "  inflating: ./data/lra-pathfinder32-curv_contour_length_9.test.pickle  \n",
            "  inflating: ./data/lra-image.train.pickle  \n",
            "  inflating: ./data/lra-image.test.pickle  \n",
            "  inflating: ./data/lra-pathfinder32-curv_contour_length_14.dev.pickle  \n",
            "  inflating: ./data/lra-retrieval.dev.pickle  \n",
            "  inflating: ./data/lra-pathfinder32-curv_contour_length_9.train.pickle  \n",
            "  inflating: ./data/lra-listops.dev.pickle  \n",
            "  inflating: ./data/lra-pathfinder32-curv_baseline.dev.pickle  \n",
            "  inflating: ./data/lra-text.dev.pickle  \n",
            "  inflating: ./data/lra-listops.train.pickle  \n",
            "  inflating: ./data/lra-listops.test.pickle  \n",
            "  inflating: ./data/lra-image.dev.pickle  \n",
            "  inflating: ./data/lra-retrieval.test.pickle  \n",
            "  inflating: ./data/lra-retrieval.train.pickle  \n",
            "  inflating: ./data/lra-pathfinder32-curv_contour_length_14.test.pickle  \n",
            "  inflating: ./data/lra-text.train.pickle  \n",
            "  inflating: ./data/lra-pathfinder32-curv_contour_length_9.dev.pickle  \n",
            "  inflating: ./data/lra-text.test.pickle  \n",
            "  inflating: ./data/lra-pathfinder32-curv_contour_length_14.train.pickle  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm 'lra_preprocessed2.zip'"
      ],
      "metadata": {
        "id": "bF0JDVMoUJO6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r 'requirements.txt'"
      ],
      "metadata": {
        "id": "FTHuMCvVmHrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --mode train --attn mlp --task lra-listops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F3_QBcgmEYg",
        "outputId": "47a23dc0-36b3-4501-c1e0-acf6573b7b0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-31 17:30:11.594840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-31 17:30:12.461251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\n",
            "    {\n",
            "        \"learn_pos_emb\": true,\n",
            "        \"tied_weights\": false,\n",
            "        \"embedding_dim\": 64,\n",
            "        \"transformer_dim\": 64,\n",
            "        \"transformer_hidden_dim\": 128,\n",
            "        \"head_dim\": 32,\n",
            "        \"num_head\": 2,\n",
            "        \"num_layers\": 2,\n",
            "        \"vocab_size\": 32,\n",
            "        \"max_seq_len\": 2048,\n",
            "        \"dropout_prob\": 0.1,\n",
            "        \"attention_dropout\": 0.1,\n",
            "        \"pooling_mode\": \"MEAN\",\n",
            "        \"num_classes\": 10,\n",
            "        \"bz_rate\": 1,\n",
            "        \"mixed_precision\": true,\n",
            "        \"attn_type\": \"mlp\",\n",
            "        \"random_seed\": 42\n",
            "    },\n",
            "    {\n",
            "        \"batch_size\": 32,\n",
            "        \"learning_rate\": 0.0001,\n",
            "        \"warmup\": 50,\n",
            "        \"lr_decay\": \"linear\",\n",
            "        \"weight_decay\": 0,\n",
            "        \"eval_frequency\": 100,\n",
            "        \"num_train_steps\": 1000,\n",
            "        \"num_init_steps\": 200,\n",
            "        \"num_eval_steps\": 62,\n",
            "        \"patience\": 10\n",
            "    }\n",
            "]\n",
            "ModelForSC(\n",
            "  (model): Model(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(32, 64)\n",
            "      (position_embeddings): Embedding(2048, 64)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_0): TransformerLayer(\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (mha): Attention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (attn): MLPAttention(\n",
            "          (drop_attn): Dropout(p=0.1, inplace=False)\n",
            "          (W_x): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (nnet): Sequential(\n",
            "            (0): Linear(in_features=32, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (ff): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "        (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (4): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (transformer_1): TransformerLayer(\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (mha): Attention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (attn): MLPAttention(\n",
            "          (drop_attn): Dropout(p=0.1, inplace=False)\n",
            "          (W_x): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "          (nnet): Sequential(\n",
            "            (0): Linear(in_features=32, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (ff): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "        (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (4): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (seq_classifer): SCHead(\n",
            "    (mlpblock): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=128, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "parameter_size: [torch.Size([32, 64]), torch.Size([2048, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([2048, 32]), torch.Size([2048]), torch.Size([2048, 2048]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([2048, 32]), torch.Size([2048]), torch.Size([2048, 2048]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "num_parameter: 8750218\n",
            "Loaded ./data/lra_processed/lra-listops.train.pickle... size=96000\n",
            "Loaded ./data/lra_processed/lra-listops.dev.pickle... size=2000\n",
            "Loaded ./data/lra_processed/lra-listops.test.pickle... size=2000\n",
            "accumu_steps=1\n",
            "{'t': 206.8364, 'loss': 2.299, 'accu': 0.105, 'best_accu': 0.105, 'component': 'train'}\n",
            "{'t': 0.3946, 'loss': 2.2731, 'accu': 0.1658, 'best_accu': 0.1658, 'component': 'dev'}\n",
            "{'t': 209.4976, 'loss': 2.2632, 'accu': 0.1697, 'best_accu': 0.1697, 'component': 'train'}\n",
            "{'t': 0.387, 'loss': 2.2635, 'accu': 0.1709, 'best_accu': 0.1709, 'component': 'dev'}\n",
            "{'t': 208.3922, 'loss': 2.2571, 'accu': 0.1631, 'best_accu': 0.1697, 'component': 'train'}\n",
            "{'t': 0.3897, 'loss': 2.2622, 'accu': 0.1613, 'best_accu': 0.1709, 'component': 'dev'}\n",
            "{'t': 208.3344, 'loss': 2.2517, 'accu': 0.1719, 'best_accu': 0.1719, 'component': 'train'}\n",
            "{'t': 0.3907, 'loss': 2.2644, 'accu': 0.1482, 'best_accu': 0.1709, 'component': 'dev'}\n",
            "{'t': 209.633, 'loss': 2.2453, 'accu': 0.1697, 'best_accu': 0.1719, 'component': 'train'}\n",
            "{'t': 0.3958, 'loss': 2.2592, 'accu': 0.1628, 'best_accu': 0.1709, 'component': 'dev'}\n",
            "{'t': 207.6473, 'loss': 2.2597, 'accu': 0.1731, 'best_accu': 0.1731, 'component': 'train'}\n",
            "{'t': 0.3839, 'loss': 2.26, 'accu': 0.1603, 'best_accu': 0.1709, 'component': 'dev'}\n",
            "{'t': 208.188, 'loss': 2.2571, 'accu': 0.1666, 'best_accu': 0.1731, 'component': 'train'}\n",
            "{'t': 0.387, 'loss': 2.258, 'accu': 0.1699, 'best_accu': 0.1709, 'component': 'dev'}\n",
            "{'t': 208.3616, 'loss': 2.2544, 'accu': 0.1659, 'best_accu': 0.1731, 'component': 'train'}\n",
            "{'t': 0.3834, 'loss': 2.2613, 'accu': 0.1527, 'best_accu': 0.1709, 'component': 'dev'}\n",
            "{'t': 208.3051, 'loss': 2.2577, 'accu': 0.1691, 'best_accu': 0.1731, 'component': 'train'}\n",
            "best model saved: step =  899 dev accu =  0.17590725806451613\n",
            "{'t': 0.4011, 'loss': 2.2587, 'accu': 0.1759, 'best_accu': 0.1759, 'component': 'dev'}\n",
            "{'t': 207.6237, 'loss': 2.2502, 'accu': 0.175, 'best_accu': 0.175, 'component': 'train'}\n",
            "{'t': 0.4179, 'loss': 2.2559, 'accu': 0.1719, 'best_accu': 0.1759, 'component': 'dev'}\n",
            "total training step (k): 1.0\n",
            "total training time (s): 2532\n",
            "peak memory usage (MB): 8795\n",
            "loading the best model from: ./checkpoints-42/test.42.model\n",
            "{'t': 0.3825, 'loss': 2.243, 'accu': 0.185, 'best_accu': 0.185, 'component': 'test'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r '/content/drive/MyDrive/skyformer/Skyformer/src/log-42'"
      ],
      "metadata": {
        "id": "I0HWxdiKoczU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r '/content/drive/MyDrive/skyformer/Skyformer/src/checkpoints-42'"
      ],
      "metadata": {
        "id": "HVBALVgio08y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --mode train --attn softmax --task lra-listops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-fXZr1IoxrR",
        "outputId": "2a7cccfa-72ce-43d9-bc69-a6f1ab5772fa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-31 18:25:09.711642: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-31 18:25:10.628685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "[\n",
            "    {\n",
            "        \"learn_pos_emb\": true,\n",
            "        \"tied_weights\": false,\n",
            "        \"embedding_dim\": 64,\n",
            "        \"transformer_dim\": 64,\n",
            "        \"transformer_hidden_dim\": 128,\n",
            "        \"head_dim\": 32,\n",
            "        \"num_head\": 2,\n",
            "        \"num_layers\": 2,\n",
            "        \"vocab_size\": 32,\n",
            "        \"max_seq_len\": 2048,\n",
            "        \"dropout_prob\": 0.1,\n",
            "        \"attention_dropout\": 0.1,\n",
            "        \"pooling_mode\": \"MEAN\",\n",
            "        \"num_classes\": 10,\n",
            "        \"bz_rate\": 1,\n",
            "        \"mixed_precision\": true,\n",
            "        \"attn_type\": \"softmax\",\n",
            "        \"random_seed\": 42\n",
            "    },\n",
            "    {\n",
            "        \"batch_size\": 32,\n",
            "        \"learning_rate\": 0.0001,\n",
            "        \"warmup\": 50,\n",
            "        \"lr_decay\": \"linear\",\n",
            "        \"weight_decay\": 0,\n",
            "        \"eval_frequency\": 100,\n",
            "        \"num_train_steps\": 1000,\n",
            "        \"num_init_steps\": 200,\n",
            "        \"num_eval_steps\": 62,\n",
            "        \"patience\": 10\n",
            "    }\n",
            "]\n",
            "ModelForSC(\n",
            "  (model): Model(\n",
            "    (embeddings): Embeddings(\n",
            "      (word_embeddings): Embedding(32, 64)\n",
            "      (position_embeddings): Embedding(2048, 64)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_0): TransformerLayer(\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (mha): Attention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (attn): SoftmaxAttention(\n",
            "          (drop_attn): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ff): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "        (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (4): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (transformer_1): TransformerLayer(\n",
            "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (mha): Attention(\n",
            "        (W_q): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_k): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (W_v): Linear(in_features=64, out_features=64, bias=True)\n",
            "        (attn): SoftmaxAttention(\n",
            "          (drop_attn): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ff): Linear(in_features=64, out_features=64, bias=True)\n",
            "      )\n",
            "      (dropout1): Dropout(p=0.1, inplace=False)\n",
            "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "      (mlpblock): Sequential(\n",
            "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "        (1): GELU(approximate='none')\n",
            "        (2): Dropout(p=0.1, inplace=False)\n",
            "        (3): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (4): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (seq_classifer): SCHead(\n",
            "    (mlpblock): Sequential(\n",
            "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=128, out_features=10, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "parameter_size: [torch.Size([32, 64]), torch.Size([2048, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64, 64]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([64, 128]), torch.Size([64]), torch.Size([64]), torch.Size([64]), torch.Size([128, 64]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "num_parameter: 209802\n",
            "Loaded ./data/lra_processed/lra-listops.train.pickle... size=96000\n",
            "Loaded ./data/lra_processed/lra-listops.dev.pickle... size=2000\n",
            "Loaded ./data/lra_processed/lra-listops.test.pickle... size=2000\n",
            "accumu_steps=1\n",
            "{'t': 37.1264, 'loss': 2.2772, 'accu': 0.1412, 'best_accu': 0.1412, 'component': 'train'}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
            "{'t': 0.4554, 'loss': 2.2624, 'accu': 0.1719, 'best_accu': 0.1719, 'component': 'dev'}\n",
            "{'t': 37.1163, 'loss': 2.2559, 'accu': 0.1666, 'best_accu': 0.1666, 'component': 'train'}\n",
            "{'t': 0.3887, 'loss': 2.2594, 'accu': 0.1709, 'best_accu': 0.1719, 'component': 'dev'}\n",
            "{'t': 37.7753, 'loss': 2.2534, 'accu': 0.1678, 'best_accu': 0.1678, 'component': 'train'}\n",
            "{'t': 0.3886, 'loss': 2.26, 'accu': 0.1613, 'best_accu': 0.1719, 'component': 'dev'}\n",
            "{'t': 37.7839, 'loss': 2.2484, 'accu': 0.1781, 'best_accu': 0.1781, 'component': 'train'}\n",
            "{'t': 0.4391, 'loss': 2.2636, 'accu': 0.1699, 'best_accu': 0.1719, 'component': 'dev'}\n",
            "{'t': 37.7732, 'loss': 2.2434, 'accu': 0.1722, 'best_accu': 0.1781, 'component': 'train'}\n",
            "{'t': 0.4576, 'loss': 2.2574, 'accu': 0.1628, 'best_accu': 0.1719, 'component': 'dev'}\n",
            "{'t': 37.7809, 'loss': 2.2579, 'accu': 0.1731, 'best_accu': 0.1781, 'component': 'train'}\n",
            "{'t': 0.4869, 'loss': 2.258, 'accu': 0.1603, 'best_accu': 0.1719, 'component': 'dev'}\n",
            "{'t': 37.7391, 'loss': 2.2556, 'accu': 0.1706, 'best_accu': 0.1781, 'component': 'train'}\n",
            "{'t': 0.5033, 'loss': 2.2558, 'accu': 0.1699, 'best_accu': 0.1719, 'component': 'dev'}\n",
            "{'t': 37.7665, 'loss': 2.2533, 'accu': 0.1678, 'best_accu': 0.1781, 'component': 'train'}\n",
            "{'t': 0.491, 'loss': 2.2612, 'accu': 0.1568, 'best_accu': 0.1719, 'component': 'dev'}\n",
            "{'t': 37.8457, 'loss': 2.2585, 'accu': 0.1641, 'best_accu': 0.1781, 'component': 'train'}\n",
            "best model saved: step =  899 dev accu =  0.1743951612903226\n",
            "{'t': 0.4462, 'loss': 2.2582, 'accu': 0.1744, 'best_accu': 0.1744, 'component': 'dev'}\n",
            "{'t': 37.7567, 'loss': 2.2494, 'accu': 0.1819, 'best_accu': 0.1819, 'component': 'train'}\n",
            "{'t': 0.4044, 'loss': 2.2558, 'accu': 0.1704, 'best_accu': 0.1744, 'component': 'dev'}\n",
            "total training step (k): 1.0\n",
            "total training time (s): 436\n",
            "peak memory usage (MB): 4474\n",
            "loading the best model from: ./checkpoints-42/test.42.model\n",
            "{'t': 0.4775, 'loss': 2.2426, 'accu': 0.1789, 'best_accu': 0.1789, 'component': 'test'}\n"
          ]
        }
      ]
    }
  ]
}